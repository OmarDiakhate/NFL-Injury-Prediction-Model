{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05765306-018c-4f16-b2f8-914f92103ba5",
   "metadata": {},
   "source": [
    "# Project 3 - Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45498519-e7e8-473f-826a-4e83a47754db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1fa78a0-58b5-4e0e-abb1-9dcc1ff03d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training, validation, and testing dataframes\n",
    "train = pd.read_csv('/Users/omar/Desktop/DSCI 372M/Projects/project3/injury_train.csv')\n",
    "val = pd.read_csv('/Users/omar/Desktop/DSCI 372M/Projects/project3/injury_val.csv')\n",
    "test = pd.read_csv('/Users/omar/Desktop/DSCI 372M/Projects/project3/injury_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2e348-d101-4248-82f2-316ca010d9eb",
   "metadata": {},
   "source": [
    "## Logistic Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec3a9-a2b3-447a-8372-e94305f1777c",
   "metadata": {},
   "source": [
    "For logistic classification, I have chosen the target variable to predict a player's game availability ('out' or 'not out'). For the sake of this model, I will classify 'Out' and 'Doubtful' as a player being out, game availability wise, as a player very rarley ends up playing when listed as doubtful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc19b9e2-8cdb-4ebb-87c7-84960cd3ae57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Classification Prediction Accuracies:\n",
      "  Training Accuracy: 0.9220330841514306\n",
      "  Validation Accuracy: 0.8726534043149341\n",
      "  Test Accuracy: 0.8670495937237321\n"
     ]
    }
   ],
   "source": [
    "# Function 'is_out' that adds the binary target column (is_out) and returns the resulting dataframe\n",
    "def is_out(df):\n",
    "    df['is_out'] = df['report_status'].apply(lambda x: 1 if x in ['Out', 'Doubtful'] else 0) # 0 = playing, 1 = out (not playing)\n",
    "    return df\n",
    "\n",
    "# Apply 'is_out' function to train_df, val_df, and test_df\n",
    "train_df = is_out(train)\n",
    "val_df = is_out(val)\n",
    "test_df = is_out(test)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train = train_df.drop(['is_out', 'report_status', 'gsis_id', 'player_name', 'date_modified'], axis=1)\n",
    "y_train = train_df['is_out']\n",
    "\n",
    "X_val = val_df.drop(['is_out', 'report_status', 'gsis_id', 'player_name', 'date_modified'], axis=1)\n",
    "y_val = val_df['is_out']\n",
    "\n",
    "X_test = test_df.drop(['is_out', 'report_status', 'gsis_id', 'player_name', 'date_modified'], axis=1)\n",
    "y_test = test_df['is_out']\n",
    "\n",
    "# Separate categorical features\n",
    "categorical = ['position', 'team', 'game_type', 'report_injury', 'practice_status']\n",
    "\n",
    "X_train_cat = train_df[categorical]\n",
    "X_val_cat = val_df[categorical]\n",
    "X_test_cat = test_df[categorical]\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform training data \n",
    "ohe.fit(X_train_cat)\n",
    "X_train_encoded = ohe.transform(X_train_cat)\n",
    "\n",
    "# Transform validation and test data\n",
    "X_val_encoded = ohe.transform(X_val_cat)\n",
    "X_test_encoded = ohe.transform(X_test_cat)\n",
    "\n",
    "# Get feature names and convert to dataframes\n",
    "encoded_cols = ohe.get_feature_names_out(categorical)\n",
    "\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoded_cols)\n",
    "X_val_encoded_df = pd.DataFrame(X_val_encoded, columns=encoded_cols)\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoded_cols)\n",
    "\n",
    "# Separate numerical features\n",
    "num_cols = ['week', 'year']\n",
    "X_train_num = train_df[num_cols].reset_index(drop=True)\n",
    "X_val_num = val_df[num_cols].reset_index(drop=True)\n",
    "X_test_num = test_df[num_cols].reset_index(drop=True)\n",
    "\n",
    "# Initialize StandardScaler and scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_num_df = pd.DataFrame(scaler.fit_transform(X_train_num), columns=num_cols)\n",
    "X_val_num_df = pd.DataFrame(scaler.transform(X_val_num), columns=num_cols)\n",
    "X_test_num_df = pd.DataFrame(scaler.transform(X_test_num), columns=num_cols)\n",
    "\n",
    "# Combine (concatenate) scaled numerical and encoded categorical features\n",
    "X_train_processed = pd.concat([X_train_num_df, X_train_encoded_df], axis=1)\n",
    "X_val_processed = pd.concat([X_val_num_df, X_val_encoded_df], axis=1)\n",
    "X_test_processed = pd.concat([X_test_num_df, X_test_encoded_df], axis=1)\n",
    "\n",
    "# Train logistic regression \n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Final evaluation on all sets\n",
    "y_pred_train = model.predict(X_train_processed)\n",
    "y_pred_val = model.predict(X_val_processed)\n",
    "y_pred_test = model.predict(X_test_processed)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Logistic Classification Prediction Accuracies:\")\n",
    "print(f\"  Training Accuracy: {train_accuracy}\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9fa27d-b01c-4968-b8dd-f88a2d62ef32",
   "metadata": {},
   "source": [
    "## Support Vector Machine:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a706ae-54b3-436d-9dee-3938d278bcc5",
   "metadata": {},
   "source": [
    "For support vector machines, I will utilize the already processed training, validation, and test sets from the logistic classification section of the project (X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test). The two kernel tricks I have chosen to use for this are a Radial Basis Function (RBF) kernel, as well as a polynomial kernel. I will also use GridSearchCV in order to tune the hyperparameters simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "103d7fff-47d8-4f60-9381-94944b76c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBF Kernel Prediction Accuracies:\n",
      "  Training Accuracy: 0.9214026239154582\n",
      "  Validation Accuracy: 0.8730736901092743\n",
      "  Test Accuracy: 0.8711123564023536\n",
      "  Best Parameters: {'C': 10, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# 1. SVM with RBF Kernel ----------------------------------------------------------------------------------------\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "rbf_param_grid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.1]}\n",
    "\n",
    "# GridSearchCV with SVC (RBF kernel) and the hyperparameter grid (2-fold cross-validation)\n",
    "rbf_grid = GridSearchCV(SVC(kernel='rbf'), rbf_param_grid, cv=2)\n",
    "\n",
    "# Train GridSearchCV on training data\n",
    "rbf_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get best estimator found by GridSearchCV\n",
    "rbf_best_svm = rbf_grid.best_estimator_\n",
    "\n",
    "# Evaluation on all sets\n",
    "rbf_y_pred_train = rbf_best_svm.predict(X_train_processed)\n",
    "rbf_y_pred_val = rbf_best_svm.predict(X_val_processed)\n",
    "rbf_y_pred_test = rbf_best_svm.predict(X_test_processed)\n",
    "\n",
    "print(\"RBF Kernel Prediction Accuracies:\")\n",
    "print(f\"  Training Accuracy: {accuracy_score(y_train, rbf_y_pred_train)}\")\n",
    "print(f\"  Validation Accuracy: {accuracy_score(y_val, rbf_y_pred_val)}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, rbf_y_pred_test)}\")\n",
    "print(f\"  Best Parameters: {rbf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b189df1-e2f6-4e53-be76-9e5f7b56798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Kernel Prediction Accuracies:\n",
      "  Training Accuracy: 0.9213726019994596\n",
      "  Validation Accuracy: 0.8711123564023536\n",
      "  Test Accuracy: 0.8607453068086298\n",
      "  Best Parameters: {'C': 0.1, 'degree': 2}\n"
     ]
    }
   ],
   "source": [
    "# 2. SVM with Polynomial Kernel ------------------------------------------------------------------------------------\n",
    "\n",
    "# Define hyperparameter grid for GridSearchCV\n",
    "poly_param_grid = {'C': [0.1, 1, 10], 'degree': [2, 3]}\n",
    "\n",
    "# GridSearchCV with SVC (polynomial kernel) and the hyperparameter grid (2-fold cross-validation)\n",
    "poly_grid = GridSearchCV(SVC(kernel='poly'), poly_param_grid, cv=2)\n",
    "\n",
    "# Train GridSearchCV on training data\n",
    "poly_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get best estimator found by GridSearchCV\n",
    "poly_best_svm = poly_grid.best_estimator_\n",
    "\n",
    "# Evaluation on all sets\n",
    "poly_y_pred_train = poly_best_svm.predict(X_train_processed)\n",
    "poly_y_pred_val = poly_best_svm.predict(X_val_processed)\n",
    "poly_y_pred_test = poly_best_svm.predict(X_test_processed)\n",
    "\n",
    "print(\"Polynomial Kernel Prediction Accuracies:\")\n",
    "print(f\"  Training Accuracy: {accuracy_score(y_train, poly_y_pred_train)}\")\n",
    "print(f\"  Validation Accuracy: {accuracy_score(y_val, poly_y_pred_val)}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, poly_y_pred_test)}\")\n",
    "print(f\"  Best Parameters: {poly_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098c17a-294b-47b5-94c5-6b0e29225636",
   "metadata": {},
   "source": [
    "## Decision Trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5b007-9ef9-463a-9536-429469b94f6e",
   "metadata": {},
   "source": [
    "For decision trees, I will utilize the already processed training, validation, and test sets from the logistic classification and SVM sections of the project (X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test). I will also use GridSearchCV for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf19cb32-ab3c-4e5b-bf90-028cec1046a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Prediction Accuracies:\n",
      "  Training Accuracy: 0.9982287069560779\n",
      "  Validation Accuracy: 0.7975623423928271\n",
      "  Test Accuracy: 0.7922387223311852\n",
      "  Best Parameters: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for GridSearchCV\n",
    "dt_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],  # max_depth values\n",
    "    'min_samples_split': [2, 5, 10],  # min_samples_split values\n",
    "    'min_samples_leaf': [1, 2, 4]  # min_samples_leaf values\n",
    "}\n",
    "\n",
    "# GridSearchCV with DecisionTreeClassifier and the hyperparameter grid (3-fold cross-validation)\n",
    "dt_grid = GridSearchCV(DecisionTreeClassifier(), dt_param_grid, cv=3)\n",
    "\n",
    "# Train decision tree model on training data\n",
    "dt_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get best estimator found by GridSearchCV\n",
    "best_dt = dt_grid.best_estimator_\n",
    "\n",
    "# Evaluation on all sets\n",
    "dt_y_pred_train = dt.predict(X_train_processed)\n",
    "dt_y_pred_val = dt.predict(X_val_processed)\n",
    "dt_y_pred_test = dt.predict(X_test_processed)\n",
    "\n",
    "print(\"Decision Tree Prediction Accuracies:\")\n",
    "print(f\"  Training Accuracy: {accuracy_score(y_train, dt_y_pred_train)}\")\n",
    "print(f\"  Validation Accuracy: {accuracy_score(y_val, dt_y_pred_val)}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, dt_y_pred_test)}\")\n",
    "print(f\"  Best Parameters: {dt_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440638f-c16a-4934-8d04-da932d3fe889",
   "metadata": {},
   "source": [
    "## Random Forests:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267d616-298f-4b4f-b9db-26a62a8b485a",
   "metadata": {},
   "source": [
    "For random forests, I will utilize the already processed training, validation, and test sets from the logistic classification, SVM, and decision tree sections of the project (X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test). I will also use GridSearchCV for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fd32448-66dd-439d-8861-f2d8a677c3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Prediction Accuracies:\n",
      "  Training Accuracy: 0.9229937854633883\n",
      "  Validation Accuracy: 0.8645278789576912\n",
      "  Test Accuracy: 0.8613056878677501\n",
      "  Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# GridSearchCV with RandomForestsClassifier and the hyperparameter grid (3-fold cross-validation)\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=3)\n",
    "\n",
    "# Train decision tree model on training data\n",
    "rf_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get best estimator found by GridSearchCV\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# Evaluation on all sets\n",
    "rf_y_pred_train = best_rf.predict(X_train_processed)\n",
    "rf_y_pred_val = best_rf.predict(X_val_processed)\n",
    "rf_y_pred_test = best_rf.predict(X_test_processed)\n",
    "\n",
    "print(\"Random Forest Prediction Accuracies:\")\n",
    "print(f\"  Training Accuracy: {accuracy_score(y_train, rf_y_pred_train)}\")\n",
    "print(f\"  Validation Accuracy: {accuracy_score(y_val, rf_y_pred_val)}\")\n",
    "print(f\"  Test Accuracy: {accuracy_score(y_test, rf_y_pred_test)}\")\n",
    "print(f\"  Best Parameters: {rf_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14bd5c-3a57-499e-a825-7500024b8293",
   "metadata": {},
   "source": [
    "## Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679cdce-4d43-4a9b-80b0-90b768eb4ee7",
   "metadata": {},
   "source": [
    "For decision trees, I will utilize the already processed training, validation, and test sets from the logistic classification and SVM sections of the project (X_train_processed, X_val_processed, X_test_processed, y_train, y_val, y_test). I have chosen to use scikit-learn's MLPClassifier (Multi-Layer Perceptron Classifier) for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd84fbdd-2c81-43e1-99dc-4675978b33f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Prediction Accuracies (Averaged over seeds):\n",
      "  Average Training Accuracy: 0.9772914227385993\n",
      "  Average Validation Accuracy: 0.7966096945923227\n",
      "  Average Test Accuracy: 0.755505743905856\n"
     ]
    }
   ],
   "source": [
    "# Define random seeds\n",
    "random_seeds = [42, 123, 7, 99, 1000]\n",
    "num_seeds = len(random_seeds)\n",
    "\n",
    "# Initialize lists to store accuracies for each set\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training/Evaluation Loop\n",
    "for seed in random_seeds:\n",
    "    # MLPClassifier with specific random seed\n",
    "    mlp = MLPClassifier(random_state=seed, max_iter=1000)\n",
    "\n",
    "    # Train MLPClassifier on processed training data\n",
    "    mlp.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Evaluation on all sets\n",
    "    y_pred_train = mlp.predict(X_train_processed)\n",
    "    y_pred_val = mlp.predict(X_val_processed)\n",
    "    y_pred_test = mlp.predict(X_test_processed)\n",
    "\n",
    "    # Calculate accuracy scores and append to respective lists\n",
    "    train_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "    val_accuracies.append(accuracy_score(y_val, y_pred_val))\n",
    "    test_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "# Average accuracies across all random seeds\n",
    "avg_train_accuracy = np.mean(train_accuracies)\n",
    "avg_val_accuracy = np.mean(val_accuracies)\n",
    "avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "print(\"Neural Network Prediction Accuracies (Averaged over seeds):\")\n",
    "print(f\"  Average Training Accuracy: {avg_train_accuracy}\")\n",
    "print(f\"  Average Validation Accuracy: {avg_val_accuracy}\")\n",
    "print(f\"  Average Test Accuracy: {avg_test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
